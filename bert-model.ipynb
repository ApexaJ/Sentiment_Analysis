{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9107347,"sourceType":"datasetVersion","datasetId":5496472},{"sourceId":9564238,"sourceType":"datasetVersion","datasetId":5828761}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-15T18:18:16.115379Z","iopub.execute_input":"2024-11-15T18:18:16.115773Z","iopub.status.idle":"2024-11-15T18:18:16.127137Z","shell.execute_reply.started":"2024-11-15T18:18:16.115732Z","shell.execute_reply":"2024-11-15T18:18:16.126144Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv\n/kaggle/input/testdataset-1/test_oJQbWVk.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:16.796872Z","iopub.execute_input":"2024-11-15T18:18:16.797610Z","iopub.status.idle":"2024-11-15T18:18:16.830954Z","shell.execute_reply.started":"2024-11-15T18:18:16.797571Z","shell.execute_reply":"2024-11-15T18:18:16.829986Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"        id  label                                              tweet\n7021  7022      0  Finally got our new washing machine! #washingm...\n3038  3039      0  - 004 - Happy Ied Mubarrak from One Day, One C...\n2224  2225      0  And I keep finding wonderful music in this alb...\n953    954      0  Finally my baby is here...#samsung #galaxy #s4...\n7873  7874      0  Welcome home bazooka! We have a great summer a...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7021</th>\n      <td>7022</td>\n      <td>0</td>\n      <td>Finally got our new washing machine! #washingm...</td>\n    </tr>\n    <tr>\n      <th>3038</th>\n      <td>3039</td>\n      <td>0</td>\n      <td>- 004 - Happy Ied Mubarrak from One Day, One C...</td>\n    </tr>\n    <tr>\n      <th>2224</th>\n      <td>2225</td>\n      <td>0</td>\n      <td>And I keep finding wonderful music in this alb...</td>\n    </tr>\n    <tr>\n      <th>953</th>\n      <td>954</td>\n      <td>0</td>\n      <td>Finally my baby is here...#samsung #galaxy #s4...</td>\n    </tr>\n    <tr>\n      <th>7873</th>\n      <td>7874</td>\n      <td>0</td>\n      <td>Welcome home bazooka! We have a great summer a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df = df.drop(columns=['id'])\ndf.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:17.544641Z","iopub.execute_input":"2024-11-15T18:18:17.545015Z","iopub.status.idle":"2024-11-15T18:18:17.556376Z","shell.execute_reply.started":"2024-11-15T18:18:17.544978Z","shell.execute_reply":"2024-11-15T18:18:17.555532Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"    label                                              tweet\n0       0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n1       0  Finally a transparant silicon case ^^ Thanks t...\n2       0  We love this! Would you go? #talk #makememorie...\n3       0  I'm wired I know I'm George I was made that wa...\n4       1  What amazing service! Apple won't even talk to...\n5       1  iPhone software update fucked up my phone big ...\n6       0  Happy for us .. #instapic #instadaily #us #son...\n7       0  New Type C charger cable #UK http://www.ebay.c...\n8       0  Bout to go shopping again listening to music #...\n9       0  Photo: #fun #selfie #pool #water #sony #camera...\n10      1  hey #apple when you make a new ipod dont make ...\n11      1  Ha! Not heavy machinery but it does what I nee...\n12      1  Contemplating giving in to the iPhone bandwago...\n13      0  I just made another crazy purchase lol my theo...\n14      1  @shaqlockholmes @sam_louise1991 the battery is...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>We love this! Would you go? #talk #makememorie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>I'm wired I know I'm George I was made that wa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>What amazing service! Apple won't even talk to...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>iPhone software update fucked up my phone big ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>Happy for us .. #instapic #instadaily #us #son...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>New Type C charger cable #UK http://www.ebay.c...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>Bout to go shopping again listening to music #...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>Photo: #fun #selfie #pool #water #sony #camera...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>hey #apple when you make a new ipod dont make ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>Ha! Not heavy machinery but it does what I nee...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>Contemplating giving in to the iPhone bandwago...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>I just made another crazy purchase lol my theo...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>@shaqlockholmes @sam_louise1991 the battery is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# lowercasing\ndf['tweet'] = df['tweet'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:18.214620Z","iopub.execute_input":"2024-11-15T18:18:18.215001Z","iopub.status.idle":"2024-11-15T18:18:18.229248Z","shell.execute_reply.started":"2024-11-15T18:18:18.214964Z","shell.execute_reply":"2024-11-15T18:18:18.228241Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# remove URL\nimport re\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:19.168863Z","iopub.execute_input":"2024-11-15T18:18:19.169552Z","iopub.status.idle":"2024-11-15T18:18:19.174422Z","shell.execute_reply.started":"2024-11-15T18:18:19.169506Z","shell.execute_reply":"2024-11-15T18:18:19.173388Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# remove URL\nimport re\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:19.987589Z","iopub.execute_input":"2024-11-15T18:18:19.988009Z","iopub.status.idle":"2024-11-15T18:18:19.992875Z","shell.execute_reply.started":"2024-11-15T18:18:19.987963Z","shell.execute_reply":"2024-11-15T18:18:19.991790Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"df['tweet']=df['tweet'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:20.880439Z","iopub.execute_input":"2024-11-15T18:18:20.880828Z","iopub.status.idle":"2024-11-15T18:18:20.922178Z","shell.execute_reply.started":"2024-11-15T18:18:20.880794Z","shell.execute_reply":"2024-11-15T18:18:20.921273Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Remove Hashtags\nimport re\ndef remove_hashtags(text):\n    return re.sub(r'#\\w*', '', text)\ndf['tweet'] = df['tweet'].apply(remove_hashtags)\nprint(df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:21.685811Z","iopub.execute_input":"2024-11-15T18:18:21.686185Z","iopub.status.idle":"2024-11-15T18:18:21.725432Z","shell.execute_reply.started":"2024-11-15T18:18:21.686149Z","shell.execute_reply":"2024-11-15T18:18:21.724462Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                          test          \n1       finally a transparant silicon case ^^ thanks t...\n2                 we love this! would you go?        ... \n3       i'm wired i know i'm george i was made that wa...\n4       what amazing service! apple won't even talk to...\n                              ...                        \n7915                                live out loud        \n7916    we would like to wish you an amazing day! make...\n7917    helping my lovely 90 year old neighbor with he...\n7918    finally got my    stay connected anytime,anywh...\n7919                       apple barcelona!!!          … \nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"contractions_dict = {\n    \"i'm\": \"I am\",\n    \"you're\": \"you are\",\n    \"he's\": \"he is\",\n    \"she's\": \"she is\",\n    \"it's\": \"it is\",\n    \"we're\": \"we are\",\n    \"they're\": \"they are\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"can't\": \"cannot\",\n    \"couldn't\": \"could not\",\n    \"shouldn't\": \"should not\",\n    \"wouldn't\": \"would not\",\n    \"haven't\": \"have not\",\n    \"hasn't\": \"has not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"would've\": \"would have\",\n    \"might've\": \"might have\",\n    \"must've\": \"must have\",\n    \"shan't\": \"shall not\",\n    \"let's\": \"let us\",\n    \"o'clock\": \"of the clock\",\n    \"y'all\": \"you all\",\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:22.683816Z","iopub.execute_input":"2024-11-15T18:18:22.684223Z","iopub.status.idle":"2024-11-15T18:18:22.690695Z","shell.execute_reply.started":"2024-11-15T18:18:22.684183Z","shell.execute_reply":"2024-11-15T18:18:22.689713Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import re\ndef expand_contractions(text):\n    # Replace contractions in the text\n    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', re.IGNORECASE)\n    expanded_text = pattern.sub(lambda x: contractions_dict[x.group(0).lower()], text)\n    \n    return expanded_text\n\n# Example usage\nsentences = [\n    \"i'm a student\",\n    \"you're welcome\",\n    \"he's happy\",\n    \"we're going to the store\",\n    \"they're excited\",\n    \"don't forget to call\",\n    \"doesn't know\",\n    \"can't believe it\",\n    \"let's go\"\n]\n\ndf['tweet'] = df['tweet'].apply(expand_contractions)\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:23.680617Z","iopub.execute_input":"2024-11-15T18:18:23.681489Z","iopub.status.idle":"2024-11-15T18:18:23.840080Z","shell.execute_reply.started":"2024-11-15T18:18:23.681439Z","shell.execute_reply":"2024-11-15T18:18:23.839163Z"},"trusted":true},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0                                          test          \n1       finally a transparant silicon case ^^ thanks t...\n2                 we love this! would you go?        ... \n3       I am wired i know I am george i was made that ...\n4       what amazing service! apple will not even talk...\n                              ...                        \n7915                                live out loud        \n7916    we would like to wish you an amazing day! make...\n7917    helping my lovely 90 year old neighbor with he...\n7918    finally got my    stay connected anytime,anywh...\n7919                       apple barcelona!!!          … \nName: tweet, Length: 7920, dtype: object"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Remove punctuation\nimport string \nexclude = string.punctuation\nexclude\ndef removePunc(text):\n    return text.translate(str.maketrans('','',exclude))\ndf['tweet']=df['tweet'].apply(removePunc)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:24.774348Z","iopub.execute_input":"2024-11-15T18:18:24.775193Z","iopub.status.idle":"2024-11-15T18:18:24.821738Z","shell.execute_reply.started":"2024-11-15T18:18:24.775155Z","shell.execute_reply":"2024-11-15T18:18:24.820791Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# HTML(df.head(5).to_html(index=False))","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:26.296425Z","iopub.execute_input":"2024-11-15T18:18:26.296828Z","iopub.status.idle":"2024-11-15T18:18:26.301509Z","shell.execute_reply.started":"2024-11-15T18:18:26.296791Z","shell.execute_reply":"2024-11-15T18:18:26.300524Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Tokenizer\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\ndf['tweet_tokens'] = df['tweet'].apply(tknzr.tokenize)\nprint(df['tweet_tokens'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:28.228787Z","iopub.execute_input":"2024-11-15T18:18:28.229175Z","iopub.status.idle":"2024-11-15T18:18:29.086241Z","shell.execute_reply.started":"2024-11-15T18:18:28.229138Z","shell.execute_reply":"2024-11-15T18:18:29.085272Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, a, transparant, silicon, case, thank...\n2                        [we, love, this, would, you, go]\n3       [i, am, wired, i, know, i, am, george, i, was,...\n4       [what, amazing, service, apple, will, not, eve...\n                              ...                        \n7915                                    [live, out, loud]\n7916    [we, would, like, to, wish, you, an, amazing, ...\n7917    [helping, my, lovely, 90, year, old, neighbor,...\n7918    [finally, got, my, stay, connected, anytimeany...\n7919                                [apple, barcelona, …]\nName: tweet_tokens, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Remove Stop Words\nfrom nltk.corpus import stopwords\ncache_english_stopwords = stopwords.words('english')\ndf['tweet'] = df['tweet_tokens'].apply(lambda x: [i for i in x if i not in cache_english_stopwords])\nprint( df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:29.405098Z","iopub.execute_input":"2024-11-15T18:18:29.405713Z","iopub.status.idle":"2024-11-15T18:18:29.584701Z","shell.execute_reply.started":"2024-11-15T18:18:29.405673Z","shell.execute_reply":"2024-11-15T18:18:29.583741Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Filter the Data after Preprocess\ndf['tweet_filtered'] = df['tweet'].apply(lambda x: ' '.join(x))\nprint( df['tweet_filtered'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:30.654402Z","iopub.execute_input":"2024-11-15T18:18:30.654808Z","iopub.status.idle":"2024-11-15T18:18:30.667357Z","shell.execute_reply.started":"2024-11-15T18:18:30.654769Z","shell.execute_reply":"2024-11-15T18:18:30.666449Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                    test\n1         finally transparant silicon case thanks uncle …\n2                                           love would go\n3                              wired know george made way\n4       amazing service apple even talk question unles...\n                              ...                        \n7915                                            live loud\n7916    would like wish amazing day make every minute ...\n7917    helping lovely 90 year old neighbor ipad morni...\n7918           finally got stay connected anytimeanywhere\n7919                                    apple barcelona …\nName: tweet_filtered, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from collections import Counter\nimport re\n\nvocabulary = set(['example', 'condition', 'during', 'generation', 'modified', 'same', 'manner'])\nword_probabilities = {'example': 0.01, 'condition': 0.02, 'during': 0.03, 'generation': 0.01, 'modified': 0.01, 'same': 0.02, 'manner': 0.01}\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\ndef edit1(word):\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes    = [L + R[1:] for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]\n    inserts    = [L + c + R for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edit2(word): \n    return (e2 for e1 in edit1(word) for e2 in edit1(e1))\n\ndef correct_spelling(word, vocabulary, word_probabilities):\n    if word in vocabulary:\n        return word\n\n    suggestions = edit1(word) or edit2(word) or [word]\n    best_guesses = [w for w in suggestions if w in vocabulary]\n    \n    if best_guesses:\n        return max(best_guesses, key=word_probabilities.get)\n    else:\n        return word\n\ndf['tweet_corrected'] = df['tweet'].apply(\n    lambda tokens: [correct_spelling(word, vocabulary, word_probabilities) for word in tokens]\n)\n\nprint(df['tweet_corrected'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:32.000048Z","iopub.execute_input":"2024-11-15T18:18:32.000887Z","iopub.status.idle":"2024-11-15T18:18:38.233581Z","shell.execute_reply.started":"2024-11-15T18:18:32.000844Z","shell.execute_reply":"2024-11-15T18:18:38.232553Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet_corrected, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"print(df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:38.235230Z","iopub.execute_input":"2024-11-15T18:18:38.235592Z","iopub.status.idle":"2024-11-15T18:18:38.243466Z","shell.execute_reply.started":"2024-11-15T18:18:38.235557Z","shell.execute_reply":"2024-11-15T18:18:38.242465Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import spacy\nimport pandas as pd\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef lemmatize_words_spacy(words):\n    doc = nlp(\" \".join(words))\n    return [token.lemma_ for token in doc]\n\ndf['tweet_lemmatized'] = df['tweet'].apply(lemmatize_words_spacy)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:38.244885Z","iopub.execute_input":"2024-11-15T18:18:38.245271Z","iopub.status.idle":"2024-11-15T18:19:25.465685Z","shell.execute_reply.started":"2024-11-15T18:18:38.245228Z","shell.execute_reply":"2024-11-15T18:19:25.464863Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Display the DataFrame with lemmatized tweets\nprint(df['tweet'])\nprint(df['tweet_lemmatized'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:19:25.467471Z","iopub.execute_input":"2024-11-15T18:19:25.467811Z","iopub.status.idle":"2024-11-15T18:19:25.478529Z","shell.execute_reply.started":"2024-11-15T18:19:25.467776Z","shell.execute_reply":"2024-11-15T18:19:25.477648Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n0                                                  [test]\n1       [finally, transparant, silicon, case, thank, u...\n2                                       [love, would, go]\n3                         [wire, know, george, make, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [help, lovely, 90, year, old, neighbor, ipad, ...\n7918     [finally, get, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet_lemmatized, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# nlp = spacy.load(\"en_core_web_sm\")\n\n# def lemmatize_chatwords(words):\n#     doc = nlp(\" \".join(words))\n#     return [token.lemma_ for token in doc]\n\n# df['chatwords_lemmatized'] = df['tweet_lemmatized'].apply(lemmatize_chatwords)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:19:31.439042Z","iopub.execute_input":"2024-11-15T18:19:31.439410Z","iopub.status.idle":"2024-11-15T18:20:17.206259Z","shell.execute_reply.started":"2024-11-15T18:19:31.439375Z","shell.execute_reply":"2024-11-15T18:20:17.205164Z"},"trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Display the DataFrame with lemmatized chat words\nprint(df['tweet'])\nprint(df['chatwords_lemmatized'])\ndf['tweet'] = df['chatwords_lemmatized'];\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:20:17.207836Z","iopub.execute_input":"2024-11-15T18:20:17.208158Z","iopub.status.idle":"2024-11-15T18:20:17.248841Z","shell.execute_reply.started":"2024-11-15T18:20:17.208124Z","shell.execute_reply":"2024-11-15T18:20:17.247940Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n0                                                  [test]\n1       [finally, transparant, silicon, case, thank, u...\n2                                       [love, would, go]\n3                         [wire, know, george, make, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [help, lovely, 90, year, old, neighbor, ipad, ...\n7918     [finally, get, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: chatwords_lemmatized, Length: 7920, dtype: object\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"      label                                              tweet  \\\n0         0                                             [test]   \n1         0  [finally, transparant, silicon, case, thank, u...   \n2         0                                  [love, would, go]   \n3         0                    [wire, know, george, make, way]   \n4         1  [amazing, service, apple, even, talk, question...   \n...     ...                                                ...   \n7915      0                                       [live, loud]   \n7916      0  [would, like, wish, amazing, day, make, every,...   \n7917      0  [help, lovely, 90, year, old, neighbor, ipad, ...   \n7918      0   [finally, get, stay, connected, anytimeanywhere]   \n7919      0                              [apple, barcelona, …]   \n\n                                           tweet_tokens  \\\n0                                                [test]   \n1     [finally, a, transparant, silicon, case, thank...   \n2                      [we, love, this, would, you, go]   \n3     [i, am, wired, i, know, i, am, george, i, was,...   \n4     [what, amazing, service, apple, will, not, eve...   \n...                                                 ...   \n7915                                  [live, out, loud]   \n7916  [we, would, like, to, wish, you, an, amazing, ...   \n7917  [helping, my, lovely, 90, year, old, neighbor,...   \n7918  [finally, got, my, stay, connected, anytimeany...   \n7919                              [apple, barcelona, …]   \n\n                                         tweet_filtered  \\\n0                                                  test   \n1       finally transparant silicon case thanks uncle …   \n2                                         love would go   \n3                            wired know george made way   \n4     amazing service apple even talk question unles...   \n...                                                 ...   \n7915                                          live loud   \n7916  would like wish amazing day make every minute ...   \n7917  helping lovely 90 year old neighbor ipad morni...   \n7918         finally got stay connected anytimeanywhere   \n7919                                  apple barcelona …   \n\n                                        tweet_corrected  \\\n0                                                [test]   \n1     [finally, transparant, silicon, case, thanks, ...   \n2                                     [love, would, go]   \n3                      [wired, know, george, made, way]   \n4     [amazing, service, apple, even, talk, question...   \n...                                                 ...   \n7915                                       [live, loud]   \n7916  [would, like, wish, amazing, day, make, every,...   \n7917  [helping, lovely, 90, year, old, neighbor, ipa...   \n7918   [finally, got, stay, connected, anytimeanywhere]   \n7919                              [apple, barcelona, …]   \n\n                                       tweet_lemmatized  \\\n0                                                [test]   \n1     [finally, transparant, silicon, case, thank, u...   \n2                                     [love, would, go]   \n3                       [wire, know, george, make, way]   \n4     [amazing, service, apple, even, talk, question...   \n...                                                 ...   \n7915                                       [live, loud]   \n7916  [would, like, wish, amazing, day, make, every,...   \n7917  [help, lovely, 90, year, old, neighbor, ipad, ...   \n7918   [finally, get, stay, connected, anytimeanywhere]   \n7919                              [apple, barcelona, …]   \n\n                                   chatwords_lemmatized  \n0                                                [test]  \n1     [finally, transparant, silicon, case, thank, u...  \n2                                     [love, would, go]  \n3                       [wire, know, george, make, way]  \n4     [amazing, service, apple, even, talk, question...  \n...                                                 ...  \n7915                                       [live, loud]  \n7916  [would, like, wish, amazing, day, make, every,...  \n7917  [help, lovely, 90, year, old, neighbor, ipad, ...  \n7918   [finally, get, stay, connected, anytimeanywhere]  \n7919                              [apple, barcelona, …]  \n\n[7920 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_tokens</th>\n      <th>tweet_filtered</th>\n      <th>tweet_corrected</th>\n      <th>tweet_lemmatized</th>\n      <th>chatwords_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[test]</td>\n      <td>[test]</td>\n      <td>test</td>\n      <td>[test]</td>\n      <td>[test]</td>\n      <td>[test]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n      <td>[finally, a, transparant, silicon, case, thank...</td>\n      <td>finally transparant silicon case thanks uncle …</td>\n      <td>[finally, transparant, silicon, case, thanks, ...</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[love, would, go]</td>\n      <td>[we, love, this, would, you, go]</td>\n      <td>love would go</td>\n      <td>[love, would, go]</td>\n      <td>[love, would, go]</td>\n      <td>[love, would, go]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[wire, know, george, make, way]</td>\n      <td>[i, am, wired, i, know, i, am, george, i, was,...</td>\n      <td>wired know george made way</td>\n      <td>[wired, know, george, made, way]</td>\n      <td>[wire, know, george, make, way]</td>\n      <td>[wire, know, george, make, way]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[what, amazing, service, apple, will, not, eve...</td>\n      <td>amazing service apple even talk question unles...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7915</th>\n      <td>0</td>\n      <td>[live, loud]</td>\n      <td>[live, out, loud]</td>\n      <td>live loud</td>\n      <td>[live, loud]</td>\n      <td>[live, loud]</td>\n      <td>[live, loud]</td>\n    </tr>\n    <tr>\n      <th>7916</th>\n      <td>0</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[we, would, like, to, wish, you, an, amazing, ...</td>\n      <td>would like wish amazing day make every minute ...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n    </tr>\n    <tr>\n      <th>7917</th>\n      <td>0</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n      <td>[helping, my, lovely, 90, year, old, neighbor,...</td>\n      <td>helping lovely 90 year old neighbor ipad morni...</td>\n      <td>[helping, lovely, 90, year, old, neighbor, ipa...</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n    </tr>\n    <tr>\n      <th>7918</th>\n      <td>0</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n      <td>[finally, got, my, stay, connected, anytimeany...</td>\n      <td>finally got stay connected anytimeanywhere</td>\n      <td>[finally, got, stay, connected, anytimeanywhere]</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n    </tr>\n    <tr>\n      <th>7919</th>\n      <td>0</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n      <td>apple barcelona …</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7920 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Dropping the unnecessary columns\ndf.drop(columns=['tweet_tokens', 'tweet_filtered','tweet_corrected','tweet_lemmatized','chatwords_lemmatized'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:20:38.478463Z","iopub.execute_input":"2024-11-15T18:20:38.478865Z","iopub.status.idle":"2024-11-15T18:20:38.491817Z","shell.execute_reply.started":"2024-11-15T18:20:38.478827Z","shell.execute_reply":"2024-11-15T18:20:38.490888Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:20:40.329055Z","iopub.execute_input":"2024-11-15T18:20:40.329447Z","iopub.status.idle":"2024-11-15T18:20:40.343659Z","shell.execute_reply.started":"2024-11-15T18:20:40.329410Z","shell.execute_reply":"2024-11-15T18:20:40.342659Z"},"trusted":true},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"      label                                              tweet\n0         0                                             [test]\n1         0  [finally, transparant, silicon, case, thank, u...\n2         0                                  [love, would, go]\n3         0                    [wire, know, george, make, way]\n4         1  [amazing, service, apple, even, talk, question...\n...     ...                                                ...\n7915      0                                       [live, loud]\n7916      0  [would, like, wish, amazing, day, make, every,...\n7917      0  [help, lovely, 90, year, old, neighbor, ipad, ...\n7918      0   [finally, get, stay, connected, anytimeanywhere]\n7919      0                              [apple, barcelona, …]\n\n[7920 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[test]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[love, would, go]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[wire, know, george, make, way]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7915</th>\n      <td>0</td>\n      <td>[live, loud]</td>\n    </tr>\n    <tr>\n      <th>7916</th>\n      <td>0</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n    </tr>\n    <tr>\n      <th>7917</th>\n      <td>0</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n    </tr>\n    <tr>\n      <th>7918</th>\n      <td>0</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n    </tr>\n    <tr>\n      <th>7919</th>\n      <td>0</td>\n      <td>[apple, barcelona, …]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7920 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"# import pandas as pd\n# import tensorflow as tf\n# from transformers import TFBertForSequenceClassification, BertTokenizer, AdamWeightDecay\n# from sklearn.model_selection import train_test_split\n# import numpy as np\n\n# # Step 1: Load the dataset\n# # Replace 'your_dataset.csv' with the act\n\n# # Step 2: Preprocess the dataset\n# # Convert labels to numpy array\n# labels = df['label'].values\n\n# # Tokenize the tweets\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# # Tokenize the text with truncation and padding\n# inputs = tokenizer(df['tweet'].tolist(), return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n\n# # Extract input_ids and attention_mask\n# input_ids = inputs['input_ids'].numpy()\n# attention_mask = inputs['attention_mask'].numpy()\n\n# # Step 3: Split data into training and validation sets\n# train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\n# train_mask, val_mask = train_test_split(attention_mask, test_size=0.1, random_state=42)\n\n# # Step 4: Define the model, optimizer, and compile the model\n# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n# optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)  # Use Hugging Face's AdamWeightDecay\n# loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# # Step 5: Disable AutoGraph warning and train the model\n# @tf.autograph.experimental.do_not_convert\n# def train_model():\n#     history = model.fit(\n#         [train_inputs, train_mask],  # Training inputs and attention mask\n#         train_labels,  # Training labels\n#         validation_data=([val_inputs, val_mask], val_labels),  # Validation set\n#         epochs=3,  # Set the number of epochs\n#         batch_size=16  # Set the batch size\n#     )\n#     return history\n\n# # Train the model\n# train_model()\n\n# # Step 6: Save the model and tokenizer\n# model.save_pretrained('./fine_tuned_model')\n# tokenizer.save_pretrained('./fine_tuned_model')\n\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import TFBertForSequenceClassification, BertTokenizer, AdamWeightDecay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support\nimport numpy as np\n\n# Step 1: Load the dataset\ndf = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')  # Replace with your actual dataset\n\n# Step 2: Preprocess the dataset\n# Convert labels to numpy array\nlabels = df['label'].values\n\n# Tokenize the tweets\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the text with truncation and padding\ninputs = tokenizer(df['tweet'].tolist(), return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n\n# Extract input_ids and attention_mask\ninput_ids = inputs['input_ids'].numpy()\nattention_mask = inputs['attention_mask'].numpy()\n\n# Step 3: Split data into training and validation sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\ntrain_mask, val_mask = train_test_split(attention_mask, test_size=0.1, random_state=42)\n\n# Step 4: Define the model, optimizer, and compile the model\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\noptimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)  # Use Hugging Face's AdamWeightDecay\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# Add precision, recall to the metrics\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n# Step 5: Train the model\n@tf.autograph.experimental.do_not_convert\ndef train_model():\n    history = model.fit(\n        [train_inputs, train_mask],  # Training inputs and attention mask\n        train_labels,  # Training labels\n        validation_data=([val_inputs, val_mask], val_labels),  # Validation set\n        epochs=3,  # Set the number of epochs\n        batch_size=16  # Set the batch size\n    )\n    return history\n\n# Train the model\ntrain_model()\n\n# Step 6: Save the model and tokenizer\nmodel.save_pretrained('./fine_tuned_model')\ntokenizer.save_pretrained('./fine_tuned_model')\n\n# Step 7: Evaluate the model and calculate F1 score\nval_preds = model.predict([val_inputs, val_mask])['logits']  # Get the model predictions\nval_preds = tf.sigmoid(val_preds).numpy()  # Apply sigmoid to convert logits to probabilities\nval_preds = (val_preds > 0.5).astype(int)  # Apply thresholding to get binary predictions\n\n# Calculate precision, recall, and F1-score\nprecision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='binary')\n\n# Print the results\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:20:43.080634Z","iopub.execute_input":"2024-11-15T18:20:43.081438Z","iopub.status.idle":"2024-11-15T18:32:10.566008Z","shell.execute_reply.started":"2024-11-15T18:20:43.081397Z","shell.execute_reply":"2024-11-15T18:32:10.565072Z"},"trusted":true},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n446/446 [==============================] - 268s 470ms/step - loss: 0.2508 - accuracy: 0.8917 - precision_1: 0.7973 - recall_1: 0.7700 - val_loss: 0.1743 - val_accuracy: 0.9280 - val_precision_1: 0.8362 - val_recall_1: 0.9108\nEpoch 2/3\n446/446 [==============================] - 195s 437ms/step - loss: 0.1785 - accuracy: 0.9310 - precision_1: 0.8526 - recall_1: 0.8809 - val_loss: 0.1793 - val_accuracy: 0.9280 - val_precision_1: 0.8645 - val_recall_1: 0.8685\nEpoch 3/3\n446/446 [==============================] - 195s 438ms/step - loss: 0.1195 - accuracy: 0.9586 - precision_1: 0.9076 - recall_1: 0.9322 - val_loss: 0.2168 - val_accuracy: 0.9242 - val_precision_1: 0.8122 - val_recall_1: 0.9343\n25/25 [==============================] - 18s 265ms/step\nPrecision: 0.8040\nRecall: 0.9437\nF1-score: 0.8683\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# !pip freeze | grep \"transformers\\|tensorflow\"\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:18:06.418294Z","iopub.status.idle":"2024-11-15T18:18:06.418700Z","shell.execute_reply.started":"2024-11-15T18:18:06.418506Z","shell.execute_reply":"2024-11-15T18:18:06.418533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}