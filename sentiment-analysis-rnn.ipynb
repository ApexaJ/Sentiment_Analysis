{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9107347,"sourceType":"datasetVersion","datasetId":5496472},{"sourceId":9447039,"sourceType":"datasetVersion","datasetId":5741632}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-15T18:04:44.384360Z","iopub.execute_input":"2024-11-15T18:04:44.384755Z","iopub.status.idle":"2024-11-15T18:04:44.394149Z","shell.execute_reply.started":"2024-11-15T18:04:44.384719Z","shell.execute_reply":"2024-11-15T18:04:44.393246Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/test-data/test_oJQbWVk.csv\n/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:44.618554Z","iopub.execute_input":"2024-11-15T18:04:44.619148Z","iopub.status.idle":"2024-11-15T18:04:44.649906Z","shell.execute_reply.started":"2024-11-15T18:04:44.619113Z","shell.execute_reply":"2024-11-15T18:04:44.649004Z"},"trusted":true},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"        id  label                                              tweet\n2248  2249      0  Happy Vibes #fruitstickers #recycleart #EfrenA...\n6182  6183      0  Soul #photography #travel #Italy #Sony #a7s #f...\n4253  4254      1  Warning! iPhone bug causing trouble with numbe...\n2699  2700      0  Gain Followers RT This MUST FOLLOW ME I FOLLOW...\n5109  5110      1  The iPad event starts now. I hate apple produc...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2248</th>\n      <td>2249</td>\n      <td>0</td>\n      <td>Happy Vibes #fruitstickers #recycleart #EfrenA...</td>\n    </tr>\n    <tr>\n      <th>6182</th>\n      <td>6183</td>\n      <td>0</td>\n      <td>Soul #photography #travel #Italy #Sony #a7s #f...</td>\n    </tr>\n    <tr>\n      <th>4253</th>\n      <td>4254</td>\n      <td>1</td>\n      <td>Warning! iPhone bug causing trouble with numbe...</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>2700</td>\n      <td>0</td>\n      <td>Gain Followers RT This MUST FOLLOW ME I FOLLOW...</td>\n    </tr>\n    <tr>\n      <th>5109</th>\n      <td>5110</td>\n      <td>1</td>\n      <td>The iPad event starts now. I hate apple produc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(df.sample(10).to_html(index=False))","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:45.530080Z","iopub.execute_input":"2024-11-15T18:04:45.530462Z","iopub.status.idle":"2024-11-15T18:04:45.539753Z","shell.execute_reply.started":"2024-11-15T18:04:45.530424Z","shell.execute_reply":"2024-11-15T18:04:45.538648Z"},"trusted":true},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2868</td>\n      <td>0</td>\n      <td>Spank, skate.. #skate #lide #day #weed #spank #iphoto #instagood #iph #instamood #iphone… https://instagram.com/p/78LKdrzCFn/</td>\n    </tr>\n    <tr>\n      <td>5373</td>\n      <td>0</td>\n      <td>Me after learning we're gonna have a baby. #Baby #Preggers #Daddy #Android #Droid #Samsung @ Wise Up http://instagr.am/p/RkeyurOIdV/</td>\n    </tr>\n    <tr>\n      <td>585</td>\n      <td>0</td>\n      <td>When you have the #iPhoneX and a #lifeproof case you can take #bomb #shower #selfies. . . . . . #selfie #gay #instagay #gayman #scruff #pride #nofilter #joy #peace … https://www.instagram.com/p/BlRwYI4nOY2/?utm_source=ig_twitter_share&amp;igshid=1jbp3x5wh43ic …</td>\n    </tr>\n    <tr>\n      <td>1548</td>\n      <td>1</td>\n      <td>eff u #Motorola for leaving my #cliqxt on 1.5! Never ever buying a Motorola product ever again! First my GPS and now this.</td>\n    </tr>\n    <tr>\n      <td>2229</td>\n      <td>0</td>\n      <td>Cool girls for you! http://ow.ly/K7W8307ox4a #ff #foodporn #dj #tokyo #apple #apps #GameofThrones #gaming #newyork #playpic.twitter.com/TDgGPbr0bU</td>\n    </tr>\n    <tr>\n      <td>5666</td>\n      <td>0</td>\n      <td>Enjoy and thanks... #apple team #thanks (with Aditya, utit, and Debbie at Lor In New Kuta Hotel) [pic] — https://path.com/p/4v7khK</td>\n    </tr>\n    <tr>\n      <td>2799</td>\n      <td>1</td>\n      <td>i hate hate hate apple with a ferociously deep burning passion. #nomusic #goingtobedpissed</td>\n    </tr>\n    <tr>\n      <td>5091</td>\n      <td>1</td>\n      <td>seriously fuck this shit. update before i kill a bitch. #apple</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0</td>\n      <td>#Kids will the #Platypus and #Friends. Why the Platypus Doesn't Sing? #Discover why. #iPhone iPad http:… (cont) http://deck.ly/~rDPxF</td>\n    </tr>\n    <tr>\n      <td>6289</td>\n      <td>0</td>\n      <td>@Samsungtweets #SamsungWishList Laptop bundle is good bcoz it has ultrabook, headphones, smart watch gear, table! Christmas #Samsung</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"df = df.drop(columns=['id'])\ndf.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:46.541120Z","iopub.execute_input":"2024-11-15T18:04:46.541975Z","iopub.status.idle":"2024-11-15T18:04:46.552804Z","shell.execute_reply.started":"2024-11-15T18:04:46.541933Z","shell.execute_reply":"2024-11-15T18:04:46.551609Z"},"trusted":true},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"    label                                              tweet\n0       0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n1       0  Finally a transparant silicon case ^^ Thanks t...\n2       0  We love this! Would you go? #talk #makememorie...\n3       0  I'm wired I know I'm George I was made that wa...\n4       1  What amazing service! Apple won't even talk to...\n5       1  iPhone software update fucked up my phone big ...\n6       0  Happy for us .. #instapic #instadaily #us #son...\n7       0  New Type C charger cable #UK http://www.ebay.c...\n8       0  Bout to go shopping again listening to music #...\n9       0  Photo: #fun #selfie #pool #water #sony #camera...\n10      1  hey #apple when you make a new ipod dont make ...\n11      1  Ha! Not heavy machinery but it does what I nee...\n12      1  Contemplating giving in to the iPhone bandwago...\n13      0  I just made another crazy purchase lol my theo...\n14      1  @shaqlockholmes @sam_louise1991 the battery is...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>We love this! Would you go? #talk #makememorie...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>I'm wired I know I'm George I was made that wa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>What amazing service! Apple won't even talk to...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>iPhone software update fucked up my phone big ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>Happy for us .. #instapic #instadaily #us #son...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>New Type C charger cable #UK http://www.ebay.c...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>Bout to go shopping again listening to music #...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>Photo: #fun #selfie #pool #water #sony #camera...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>hey #apple when you make a new ipod dont make ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>Ha! Not heavy machinery but it does what I nee...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>Contemplating giving in to the iPhone bandwago...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>I just made another crazy purchase lol my theo...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>@shaqlockholmes @sam_louise1991 the battery is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"# lowercasing\ndf['tweet'] = df['tweet'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:47.590997Z","iopub.execute_input":"2024-11-15T18:04:47.591853Z","iopub.status.idle":"2024-11-15T18:04:47.604174Z","shell.execute_reply.started":"2024-11-15T18:04:47.591813Z","shell.execute_reply":"2024-11-15T18:04:47.603281Z"},"trusted":true},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# remove URL\nimport re\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:48.578237Z","iopub.execute_input":"2024-11-15T18:04:48.578636Z","iopub.status.idle":"2024-11-15T18:04:48.583237Z","shell.execute_reply.started":"2024-11-15T18:04:48.578575Z","shell.execute_reply":"2024-11-15T18:04:48.582302Z"},"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# remove URL\nimport re\ndef remove_url(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:49.264348Z","iopub.execute_input":"2024-11-15T18:04:49.264736Z","iopub.status.idle":"2024-11-15T18:04:49.269481Z","shell.execute_reply.started":"2024-11-15T18:04:49.264700Z","shell.execute_reply":"2024-11-15T18:04:49.268570Z"},"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"code","source":"df['tweet']=df['tweet'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:49.865264Z","iopub.execute_input":"2024-11-15T18:04:49.865666Z","iopub.status.idle":"2024-11-15T18:04:49.906477Z","shell.execute_reply.started":"2024-11-15T18:04:49.865625Z","shell.execute_reply":"2024-11-15T18:04:49.905636Z"},"trusted":true},"outputs":[],"execution_count":77},{"cell_type":"code","source":"# Remove Hashtags\nimport re\ndef remove_hashtags(text):\n    return re.sub(r'#\\w*', '', text)\ndf['tweet'] = df['tweet'].apply(remove_hashtags)\nprint(df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:50.142433Z","iopub.execute_input":"2024-11-15T18:04:50.143189Z","iopub.status.idle":"2024-11-15T18:04:50.181192Z","shell.execute_reply.started":"2024-11-15T18:04:50.143151Z","shell.execute_reply":"2024-11-15T18:04:50.180384Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                          test          \n1       finally a transparant silicon case ^^ thanks t...\n2                 we love this! would you go?        ... \n3       i'm wired i know i'm george i was made that wa...\n4       what amazing service! apple won't even talk to...\n                              ...                        \n7915                                live out loud        \n7916    we would like to wish you an amazing day! make...\n7917    helping my lovely 90 year old neighbor with he...\n7918    finally got my    stay connected anytime,anywh...\n7919                       apple barcelona!!!          … \nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"contractions_dict = {\n    \"i'm\": \"I am\",\n    \"you're\": \"you are\",\n    \"he's\": \"he is\",\n    \"she's\": \"she is\",\n    \"it's\": \"it is\",\n    \"we're\": \"we are\",\n    \"they're\": \"they are\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"can't\": \"cannot\",\n    \"couldn't\": \"could not\",\n    \"shouldn't\": \"should not\",\n    \"wouldn't\": \"would not\",\n    \"haven't\": \"have not\",\n    \"hasn't\": \"has not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"would've\": \"would have\",\n    \"might've\": \"might have\",\n    \"must've\": \"must have\",\n    \"shan't\": \"shall not\",\n    \"let's\": \"let us\",\n    \"o'clock\": \"of the clock\",\n    \"y'all\": \"you all\",\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:50.369352Z","iopub.execute_input":"2024-11-15T18:04:50.369831Z","iopub.status.idle":"2024-11-15T18:04:50.376345Z","shell.execute_reply.started":"2024-11-15T18:04:50.369789Z","shell.execute_reply":"2024-11-15T18:04:50.375335Z"},"trusted":true},"outputs":[],"execution_count":79},{"cell_type":"code","source":"import re\ndef expand_contractions(text):\n    # Replace contractions in the text\n    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', re.IGNORECASE)\n    expanded_text = pattern.sub(lambda x: contractions_dict[x.group(0).lower()], text)\n    \n    return expanded_text\n\n# Example usage\nsentences = [\n    \"i'm a student\",\n    \"you're welcome\",\n    \"he's happy\",\n    \"we're going to the store\",\n    \"they're excited\",\n    \"don't forget to call\",\n    \"doesn't know\",\n    \"can't believe it\",\n    \"let's go\"\n]\n\ndf['tweet'] = df['tweet'].apply(expand_contractions)\ndf['tweet']","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:50.564410Z","iopub.execute_input":"2024-11-15T18:04:50.564757Z","iopub.status.idle":"2024-11-15T18:04:50.722491Z","shell.execute_reply.started":"2024-11-15T18:04:50.564722Z","shell.execute_reply":"2024-11-15T18:04:50.721644Z"},"trusted":true},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"0                                          test          \n1       finally a transparant silicon case ^^ thanks t...\n2                 we love this! would you go?        ... \n3       I am wired i know I am george i was made that ...\n4       what amazing service! apple will not even talk...\n                              ...                        \n7915                                live out loud        \n7916    we would like to wish you an amazing day! make...\n7917    helping my lovely 90 year old neighbor with he...\n7918    finally got my    stay connected anytime,anywh...\n7919                       apple barcelona!!!          … \nName: tweet, Length: 7920, dtype: object"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"# Remove punctuation\nimport string \nexclude = string.punctuation\nexclude\ndef removePunc(text):\n    return text.translate(str.maketrans('','',exclude))\ndf['tweet']=df['tweet'].apply(removePunc)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:50.770684Z","iopub.execute_input":"2024-11-15T18:04:50.770984Z","iopub.status.idle":"2024-11-15T18:04:50.814977Z","shell.execute_reply.started":"2024-11-15T18:04:50.770952Z","shell.execute_reply":"2024-11-15T18:04:50.814070Z"},"trusted":true},"outputs":[],"execution_count":81},{"cell_type":"code","source":"HTML(df.head(5).to_html(index=False))","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:50.949903Z","iopub.execute_input":"2024-11-15T18:04:50.950209Z","iopub.status.idle":"2024-11-15T18:04:50.957497Z","shell.execute_reply.started":"2024-11-15T18:04:50.950178Z","shell.execute_reply":"2024-11-15T18:04:50.956509Z"},"trusted":true},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <td>0</td>\n      <td>finally a transparant silicon case  thanks to my uncle      …</td>\n    </tr>\n    <tr>\n      <td>0</td>\n      <td>we love this would you go</td>\n    </tr>\n    <tr>\n      <td>0</td>\n      <td>I am wired i know I am george i was made that way</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>what amazing service apple will not even talk to me about a question i have unless i pay them 1995 for their stupid support</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"# Tokenizer\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\ndf['tweet_tokens'] = df['tweet'].apply(tknzr.tokenize)\nprint(df['tweet_tokens'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:51.181094Z","iopub.execute_input":"2024-11-15T18:04:51.181520Z","iopub.status.idle":"2024-11-15T18:04:52.023478Z","shell.execute_reply.started":"2024-11-15T18:04:51.181468Z","shell.execute_reply":"2024-11-15T18:04:52.022536Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, a, transparant, silicon, case, thank...\n2                        [we, love, this, would, you, go]\n3       [i, am, wired, i, know, i, am, george, i, was,...\n4       [what, amazing, service, apple, will, not, eve...\n                              ...                        \n7915                                    [live, out, loud]\n7916    [we, would, like, to, wish, you, an, amazing, ...\n7917    [helping, my, lovely, 90, year, old, neighbor,...\n7918    [finally, got, my, stay, connected, anytimeany...\n7919                                [apple, barcelona, …]\nName: tweet_tokens, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# Remove Stop Words\nfrom nltk.corpus import stopwords\ncache_english_stopwords = stopwords.words('english')\ndf['tweet'] = df['tweet_tokens'].apply(lambda x: [i for i in x if i not in cache_english_stopwords])\nprint( df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:52.025173Z","iopub.execute_input":"2024-11-15T18:04:52.025487Z","iopub.status.idle":"2024-11-15T18:04:52.203521Z","shell.execute_reply.started":"2024-11-15T18:04:52.025455Z","shell.execute_reply":"2024-11-15T18:04:52.202616Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"# Filter the Data after Preprocess\ndf['tweet_filtered'] = df['tweet'].apply(lambda x: ' '.join(x))\nprint( df['tweet_filtered'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:52.204815Z","iopub.execute_input":"2024-11-15T18:04:52.205126Z","iopub.status.idle":"2024-11-15T18:04:52.217481Z","shell.execute_reply.started":"2024-11-15T18:04:52.205094Z","shell.execute_reply":"2024-11-15T18:04:52.216644Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                    test\n1         finally transparant silicon case thanks uncle …\n2                                           love would go\n3                              wired know george made way\n4       amazing service apple even talk question unles...\n                              ...                        \n7915                                            live loud\n7916    would like wish amazing day make every minute ...\n7917    helping lovely 90 year old neighbor ipad morni...\n7918           finally got stay connected anytimeanywhere\n7919                                    apple barcelona …\nName: tweet_filtered, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"from collections import Counter\nimport re\n\nvocabulary = set(['example', 'condition', 'during', 'generation', 'modified', 'same', 'manner'])\nword_probabilities = {'example': 0.01, 'condition': 0.02, 'during': 0.03, 'generation': 0.01, 'modified': 0.01, 'same': 0.02, 'manner': 0.01}\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\ndef edit1(word):\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes    = [L + R[1:] for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]\n    inserts    = [L + c + R for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edit2(word): \n    return (e2 for e1 in edit1(word) for e2 in edit1(e1))\n\ndef correct_spelling(word, vocabulary, word_probabilities):\n    if word in vocabulary:\n        return word\n\n    suggestions = edit1(word) or edit2(word) or [word]\n    best_guesses = [w for w in suggestions if w in vocabulary]\n    \n    if best_guesses:\n        return max(best_guesses, key=word_probabilities.get)\n    else:\n        return word\n\ndf['tweet_corrected'] = df['tweet'].apply(\n    lambda tokens: [correct_spelling(word, vocabulary, word_probabilities) for word in tokens]\n)\n\nprint(df['tweet_corrected'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:52.219494Z","iopub.execute_input":"2024-11-15T18:04:52.219827Z","iopub.status.idle":"2024-11-15T18:04:58.567475Z","shell.execute_reply.started":"2024-11-15T18:04:52.219796Z","shell.execute_reply":"2024-11-15T18:04:58.566527Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet_corrected, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"print(df['tweet'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:58.568613Z","iopub.execute_input":"2024-11-15T18:04:58.568907Z","iopub.status.idle":"2024-11-15T18:04:58.576696Z","shell.execute_reply.started":"2024-11-15T18:04:58.568874Z","shell.execute_reply":"2024-11-15T18:04:58.575583Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"import spacy\nimport pandas as pd\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef lemmatize_words_spacy(words):\n    doc = nlp(\" \".join(words))\n    return [token.lemma_ for token in doc]\n\ndf['tweet_lemmatized'] = df['tweet'].apply(lemmatize_words_spacy)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:04:58.578064Z","iopub.execute_input":"2024-11-15T18:04:58.578482Z","iopub.status.idle":"2024-11-15T18:05:44.560658Z","shell.execute_reply.started":"2024-11-15T18:04:58.578432Z","shell.execute_reply":"2024-11-15T18:05:44.559719Z"},"trusted":true},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# Display the DataFrame with lemmatized tweets\nprint(df['tweet'])\nprint(df['tweet_lemmatized'])","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:05:44.563381Z","iopub.execute_input":"2024-11-15T18:05:44.563738Z","iopub.status.idle":"2024-11-15T18:05:44.574424Z","shell.execute_reply.started":"2024-11-15T18:05:44.563704Z","shell.execute_reply":"2024-11-15T18:05:44.573338Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n0                                                  [test]\n1       [finally, transparant, silicon, case, thank, u...\n2                                       [love, would, go]\n3                         [wire, know, george, make, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [help, lovely, 90, year, old, neighbor, ipad, ...\n7918     [finally, get, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet_lemmatized, Length: 7920, dtype: object\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\ndef lemmatize_chatwords(words):\n    doc = nlp(\" \".join(words))\n    return [token.lemma_ for token in doc]\n\ndf['chatwords_lemmatized'] = df['tweet_lemmatized'].apply(lemmatize_chatwords)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:05:44.575807Z","iopub.execute_input":"2024-11-15T18:05:44.576555Z","iopub.status.idle":"2024-11-15T18:06:30.688743Z","shell.execute_reply.started":"2024-11-15T18:05:44.576491Z","shell.execute_reply":"2024-11-15T18:06:30.687850Z"},"trusted":true},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Display the DataFrame with lemmatized chat words\nprint(df['tweet'])\nprint(df['chatwords_lemmatized'])\ndf['tweet'] = df['chatwords_lemmatized'];\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:06:30.689883Z","iopub.execute_input":"2024-11-15T18:06:30.690200Z","iopub.status.idle":"2024-11-15T18:06:30.734096Z","shell.execute_reply.started":"2024-11-15T18:06:30.690166Z","shell.execute_reply":"2024-11-15T18:06:30.733004Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0                                                  [test]\n1       [finally, transparant, silicon, case, thanks, ...\n2                                       [love, would, go]\n3                        [wired, know, george, made, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [helping, lovely, 90, year, old, neighbor, ipa...\n7918     [finally, got, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: tweet, Length: 7920, dtype: object\n0                                                  [test]\n1       [finally, transparant, silicon, case, thank, u...\n2                                       [love, would, go]\n3                         [wire, know, george, make, way]\n4       [amazing, service, apple, even, talk, question...\n                              ...                        \n7915                                         [live, loud]\n7916    [would, like, wish, amazing, day, make, every,...\n7917    [help, lovely, 90, year, old, neighbor, ipad, ...\n7918     [finally, get, stay, connected, anytimeanywhere]\n7919                                [apple, barcelona, …]\nName: chatwords_lemmatized, Length: 7920, dtype: object\n","output_type":"stream"},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"      label                                              tweet  \\\n0         0                                             [test]   \n1         0  [finally, transparant, silicon, case, thank, u...   \n2         0                                  [love, would, go]   \n3         0                    [wire, know, george, make, way]   \n4         1  [amazing, service, apple, even, talk, question...   \n...     ...                                                ...   \n7915      0                                       [live, loud]   \n7916      0  [would, like, wish, amazing, day, make, every,...   \n7917      0  [help, lovely, 90, year, old, neighbor, ipad, ...   \n7918      0   [finally, get, stay, connected, anytimeanywhere]   \n7919      0                              [apple, barcelona, …]   \n\n                                           tweet_tokens  \\\n0                                                [test]   \n1     [finally, a, transparant, silicon, case, thank...   \n2                      [we, love, this, would, you, go]   \n3     [i, am, wired, i, know, i, am, george, i, was,...   \n4     [what, amazing, service, apple, will, not, eve...   \n...                                                 ...   \n7915                                  [live, out, loud]   \n7916  [we, would, like, to, wish, you, an, amazing, ...   \n7917  [helping, my, lovely, 90, year, old, neighbor,...   \n7918  [finally, got, my, stay, connected, anytimeany...   \n7919                              [apple, barcelona, …]   \n\n                                         tweet_filtered  \\\n0                                                  test   \n1       finally transparant silicon case thanks uncle …   \n2                                         love would go   \n3                            wired know george made way   \n4     amazing service apple even talk question unles...   \n...                                                 ...   \n7915                                          live loud   \n7916  would like wish amazing day make every minute ...   \n7917  helping lovely 90 year old neighbor ipad morni...   \n7918         finally got stay connected anytimeanywhere   \n7919                                  apple barcelona …   \n\n                                        tweet_corrected  \\\n0                                                [test]   \n1     [finally, transparant, silicon, case, thanks, ...   \n2                                     [love, would, go]   \n3                      [wired, know, george, made, way]   \n4     [amazing, service, apple, even, talk, question...   \n...                                                 ...   \n7915                                       [live, loud]   \n7916  [would, like, wish, amazing, day, make, every,...   \n7917  [helping, lovely, 90, year, old, neighbor, ipa...   \n7918   [finally, got, stay, connected, anytimeanywhere]   \n7919                              [apple, barcelona, …]   \n\n                                       tweet_lemmatized  \\\n0                                                [test]   \n1     [finally, transparant, silicon, case, thank, u...   \n2                                     [love, would, go]   \n3                       [wire, know, george, make, way]   \n4     [amazing, service, apple, even, talk, question...   \n...                                                 ...   \n7915                                       [live, loud]   \n7916  [would, like, wish, amazing, day, make, every,...   \n7917  [help, lovely, 90, year, old, neighbor, ipad, ...   \n7918   [finally, get, stay, connected, anytimeanywhere]   \n7919                              [apple, barcelona, …]   \n\n                                   chatwords_lemmatized  \n0                                                [test]  \n1     [finally, transparant, silicon, case, thank, u...  \n2                                     [love, would, go]  \n3                       [wire, know, george, make, way]  \n4     [amazing, service, apple, even, talk, question...  \n...                                                 ...  \n7915                                       [live, loud]  \n7916  [would, like, wish, amazing, day, make, every,...  \n7917  [help, lovely, 90, year, old, neighbor, ipad, ...  \n7918   [finally, get, stay, connected, anytimeanywhere]  \n7919                              [apple, barcelona, …]  \n\n[7920 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>tweet_tokens</th>\n      <th>tweet_filtered</th>\n      <th>tweet_corrected</th>\n      <th>tweet_lemmatized</th>\n      <th>chatwords_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[test]</td>\n      <td>[test]</td>\n      <td>test</td>\n      <td>[test]</td>\n      <td>[test]</td>\n      <td>[test]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n      <td>[finally, a, transparant, silicon, case, thank...</td>\n      <td>finally transparant silicon case thanks uncle …</td>\n      <td>[finally, transparant, silicon, case, thanks, ...</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[love, would, go]</td>\n      <td>[we, love, this, would, you, go]</td>\n      <td>love would go</td>\n      <td>[love, would, go]</td>\n      <td>[love, would, go]</td>\n      <td>[love, would, go]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[wire, know, george, make, way]</td>\n      <td>[i, am, wired, i, know, i, am, george, i, was,...</td>\n      <td>wired know george made way</td>\n      <td>[wired, know, george, made, way]</td>\n      <td>[wire, know, george, make, way]</td>\n      <td>[wire, know, george, make, way]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[what, amazing, service, apple, will, not, eve...</td>\n      <td>amazing service apple even talk question unles...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7915</th>\n      <td>0</td>\n      <td>[live, loud]</td>\n      <td>[live, out, loud]</td>\n      <td>live loud</td>\n      <td>[live, loud]</td>\n      <td>[live, loud]</td>\n      <td>[live, loud]</td>\n    </tr>\n    <tr>\n      <th>7916</th>\n      <td>0</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[we, would, like, to, wish, you, an, amazing, ...</td>\n      <td>would like wish amazing day make every minute ...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n    </tr>\n    <tr>\n      <th>7917</th>\n      <td>0</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n      <td>[helping, my, lovely, 90, year, old, neighbor,...</td>\n      <td>helping lovely 90 year old neighbor ipad morni...</td>\n      <td>[helping, lovely, 90, year, old, neighbor, ipa...</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n    </tr>\n    <tr>\n      <th>7918</th>\n      <td>0</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n      <td>[finally, got, my, stay, connected, anytimeany...</td>\n      <td>finally got stay connected anytimeanywhere</td>\n      <td>[finally, got, stay, connected, anytimeanywhere]</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n    </tr>\n    <tr>\n      <th>7919</th>\n      <td>0</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n      <td>apple barcelona …</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n      <td>[apple, barcelona, …]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7920 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"# Dropping the unnecessary columns\ndf.drop(columns=['tweet_tokens', 'tweet_filtered','tweet_corrected','tweet_lemmatized','chatwords_lemmatized'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:06:30.735503Z","iopub.execute_input":"2024-11-15T18:06:30.736216Z","iopub.status.idle":"2024-11-15T18:06:30.748923Z","shell.execute_reply.started":"2024-11-15T18:06:30.736169Z","shell.execute_reply":"2024-11-15T18:06:30.748142Z"},"trusted":true},"outputs":[],"execution_count":92},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:06:30.750095Z","iopub.execute_input":"2024-11-15T18:06:30.750499Z","iopub.status.idle":"2024-11-15T18:06:30.766482Z","shell.execute_reply.started":"2024-11-15T18:06:30.750466Z","shell.execute_reply":"2024-11-15T18:06:30.765652Z"},"trusted":true},"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"      label                                              tweet\n0         0                                             [test]\n1         0  [finally, transparant, silicon, case, thank, u...\n2         0                                  [love, would, go]\n3         0                    [wire, know, george, make, way]\n4         1  [amazing, service, apple, even, talk, question...\n...     ...                                                ...\n7915      0                                       [live, loud]\n7916      0  [would, like, wish, amazing, day, make, every,...\n7917      0  [help, lovely, 90, year, old, neighbor, ipad, ...\n7918      0   [finally, get, stay, connected, anytimeanywhere]\n7919      0                              [apple, barcelona, …]\n\n[7920 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[test]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[finally, transparant, silicon, case, thank, u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[love, would, go]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>[wire, know, george, make, way]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>[amazing, service, apple, even, talk, question...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7915</th>\n      <td>0</td>\n      <td>[live, loud]</td>\n    </tr>\n    <tr>\n      <th>7916</th>\n      <td>0</td>\n      <td>[would, like, wish, amazing, day, make, every,...</td>\n    </tr>\n    <tr>\n      <th>7917</th>\n      <td>0</td>\n      <td>[help, lovely, 90, year, old, neighbor, ipad, ...</td>\n    </tr>\n    <tr>\n      <th>7918</th>\n      <td>0</td>\n      <td>[finally, get, stay, connected, anytimeanywhere]</td>\n    </tr>\n    <tr>\n      <th>7919</th>\n      <td>0</td>\n      <td>[apple, barcelona, …]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7920 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":93},{"cell_type":"code","source":"# # from sklearn.model_selection import train_test_split\n\n# # # Splitting the data into training, validation, and test sets\n# # X = df['tweet']  # Features\n# # y = df['label']  # Labels\n\n# # # Split into 80% train + validation and 20% test\n# # X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # # Split the 80% into 70% training and 10% validation\n# # X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)\n\n# # X_train, X_valid, X_test, y_train, y_valid, y_test\n\n\n# # # Assuming test data is loaded into 'test_df'\n# # test_df = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')\n\n# # # Make sure the test data has the same features as your training data\n# # X_test = test_df  # Since the test data doesn't have an output column\n\n\n\n\n\n\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n\n# # Load the dataset\n# df = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')\n\n# # Step 1: Preprocessing\n# # Lowercasing the tweets\n# df['tweet'] = df['tweet'].str.lower()\n\n# # Features and labels\n# X = df['tweet']\n# y = df['label']\n\n# # Step 2: Splitting the data\n# # 80% train + validation, 20% test\n# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # 70% train, 10% validation (from the 80% train + validation data)\n# X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)\n\n# # Step 3: Tokenization\n# max_words = 10000  # Maximum number of words to consider\n# max_sequence_length = 100  # Maximum length of sequences\n\n# tokenizer = Tokenizer(num_words=max_words)\n# tokenizer.fit_on_texts(X_train)\n\n# # Convert text to sequences of integers\n# X_train_sequences = tokenizer.texts_to_sequences(X_train)\n# X_valid_sequences = tokenizer.texts_to_sequences(X_valid)\n# X_test_sequences = tokenizer.texts_to_sequences(X_test)\n\n# # Step 4: Padding the sequences\n# X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n# X_valid_padded = pad_sequences(X_valid_sequences, maxlen=max_sequence_length)\n# X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n\n# # Step 5: Build the RNN model\n# vocab_size = max_words\n# embedding_dim = 100\n\n# model = Sequential()\n# # Embedding layer\n# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n# # SimpleRNN layer\n# model.add(SimpleRNN(100))\n# # Output layer for binary classification\n# model.add(Dense(1, activation='sigmoid'))\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# # Step 6: Train the model\n# model.fit(X_train_padded, y_train, epochs=27, batch_size=64, validation_data=(X_valid_padded, y_valid))\n\n# # Step 7: Evaluate on the test set\n# # test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n# # print(f\"Test Accuracy: {test_accuracy}\")\n# # test_loss, test_accuracy = model.evaluate(X_train_padded, y_train)\n# # print(f\"Test Accuracy: {train_accuracy}\")\n# # Step 7: Evaluate on the test set\n# test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n# print(f\"Test Accuracy: {test_accuracy}\")\n\n# # You already calculated the accuracy on the training set, so no need to reassign it.\n# train_loss, train_accuracy = model.evaluate(X_train_padded, y_train)\n# print(f\"Train Accuracy: {train_accuracy}\")\n\n\n# # If you want to predict on an unseen test dataset (test_df) that doesn't have labels:\n# # Assuming test_df is loaded separately without labels\n# test_df = pd.read_csv('/kaggle/input/test-data/test_oJQbWVk.csv')  # Replace with the actual path to the test data\n# test_df['tweet'] = test_df['tweet'].str.lower()  # Preprocess the test data\n\n# # Convert test data into sequences\n# X_test_unseen_sequences = tokenizer.texts_to_sequences(test_df['tweet'])\n# X_test_unseen_padded = pad_sequences(X_test_unseen_sequences, maxlen=max_sequence_length)\n\n# # Make predictions on the unseen test data\n# y_test_unseen_pred = model.predict(X_test_unseen_padded)\n\n# # Convert predictions to binary labels (0 or 1)\n# y_test_unseen_pred_labels = (y_test_unseen_pred > 0.5).astype(int)\n\n# # Save the predictions to a CSV file in the /kaggle/working/ directory\n# test_predictions = pd.DataFrame({'predicted_label': y_test_unseen_pred_labels.flatten()})\n# test_predictions.to_csv('/kaggle/working/test_predictions.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:06:30.767832Z","iopub.execute_input":"2024-11-15T18:06:30.768102Z","iopub.status.idle":"2024-11-15T18:06:30.776358Z","shell.execute_reply.started":"2024-11-15T18:06:30.768072Z","shell.execute_reply":"2024-11-15T18:06:30.775481Z"},"trusted":true},"outputs":[],"execution_count":94},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Embedding\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/sentimenttraindataset/train_2kmZucJ.csv')\n\n# Step 1: Preprocessing\n# Lowercasing the tweets\ndf['tweet'] = df['tweet'].str.lower()\n\n# Features and labels\nX = df['tweet']\ny = df['label']\n\n# Step 2: Splitting the data\n# 80% train + validation, 20% test\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 70% train, 10% validation (from the 80% train + validation data)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)\n\n# Step 3: Tokenization\nmax_words = 10000  # Maximum number of words to consider\nmax_sequence_length = 100  # Maximum length of sequences\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\n\n# Convert text to sequences of integers\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_valid_sequences = tokenizer.texts_to_sequences(X_valid)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Step 4: Padding the sequences\nX_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\nX_valid_padded = pad_sequences(X_valid_sequences, maxlen=max_sequence_length)\nX_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n    \n# Step 5: Build the RNN model\nvocab_size = max_words\nembedding_dim = 100\n\nmodel = Sequential()\n# Embedding layer\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n# SimpleRNN layer\nmodel.add(SimpleRNN(100))\n# Output layer for binary classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Step 6: Train the model\nmodel.fit(X_train_padded, y_train, epochs=24, batch_size=64, validation_data=(X_valid_padded, y_valid))\n\n# Step 7: Evaluate on the test set\ntest_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Evaluate accuracy on the training set as well\ntrain_loss, train_accuracy = model.evaluate(X_train_padded, y_train)\nprint(f\"Train Accuracy: {train_accuracy}\")\n\n# Step 8: Make predictions on the test set\ny_test_pred = model.predict(X_test_padded)\ny_test_pred_labels = (y_test_pred > 0.5).astype(int)\n\n# Step 9: Calculate precision, recall, and F1-score\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred_labels, average='binary')\n\n# Output the results\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1-Score: {f1}\")\n\n# Also calculate and output accuracy\naccuracy = accuracy_score(y_test, y_test_pred_labels)\nprint(f\"Accuracy: {accuracy}\")\n\n# Step 10: Predict on unseen test dataset (optional)\n# Assuming test_df is loaded separately without labels\ntest_df = pd.read_csv('/kaggle/input/test-data/test_oJQbWVk.csv')  # Replace with the actual path to the test data\ntest_df['tweet'] = test_df['tweet'].str.lower()  # Preprocess the test data\n\n# Convert test data into sequences\nX_test_unseen_sequences = tokenizer.texts_to_sequences(test_df['tweet'])\nX_test_unseen_padded = pad_sequences(X_test_unseen_sequences, maxlen=max_sequence_length)\n\n# Make predictions on the unseen test data\ny_test_unseen_pred = model.predict(X_test_unseen_padded)\n\n# Convert predictions to binary labels (0 or 1)\ny_test_unseen_pred_labels = (y_test_unseen_pred > 0.5).astype(int)\n\n# Save the predictions to a CSV file in the /kaggle/working/ directory\ntest_predictions = pd.DataFrame({'predicted_label': y_test_unseen_pred_labels.flatten()})\ntest_predictions.to_csv('/kaggle/working/test_predictions.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:09:04.065875Z","iopub.execute_input":"2024-11-15T18:09:04.066220Z","iopub.status.idle":"2024-11-15T18:09:31.300401Z","shell.execute_reply.started":"2024-11-15T18:09:04.066188Z","shell.execute_reply":"2024-11-15T18:09:31.299646Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/24\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.7782 - loss: 0.4579 - val_accuracy: 0.8573 - val_loss: 0.3088\nEpoch 2/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9152 - loss: 0.1991 - val_accuracy: 0.8687 - val_loss: 0.3032\nEpoch 3/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9802 - loss: 0.0621 - val_accuracy: 0.8737 - val_loss: 0.3714\nEpoch 4/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9984 - loss: 0.0141 - val_accuracy: 0.8699 - val_loss: 0.4471\nEpoch 5/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0060 - val_accuracy: 0.8636 - val_loss: 0.4675\nEpoch 6/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0055 - val_accuracy: 0.8649 - val_loss: 0.4582\nEpoch 7/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 0.8636 - val_loss: 0.4990\nEpoch 8/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 0.8586 - val_loss: 0.5148\nEpoch 9/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.8662 - val_loss: 0.5249\nEpoch 10/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0025 - val_accuracy: 0.8586 - val_loss: 0.5067\nEpoch 11/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.8649 - val_loss: 0.5225\nEpoch 12/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.0035 - val_accuracy: 0.8636 - val_loss: 0.5820\nEpoch 13/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9247 - loss: 0.2235 - val_accuracy: 0.8674 - val_loss: 0.3210\nEpoch 14/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9605 - loss: 0.1023 - val_accuracy: 0.8586 - val_loss: 0.3528\nEpoch 15/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9907 - loss: 0.0401 - val_accuracy: 0.8396 - val_loss: 0.4545\nEpoch 16/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9933 - loss: 0.0249 - val_accuracy: 0.8485 - val_loss: 0.5321\nEpoch 17/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0163 - val_accuracy: 0.8573 - val_loss: 0.5097\nEpoch 18/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0062 - val_accuracy: 0.8523 - val_loss: 0.5919\nEpoch 19/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9982 - loss: 0.0076 - val_accuracy: 0.8611 - val_loss: 0.5860\nEpoch 20/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9983 - loss: 0.0077 - val_accuracy: 0.8523 - val_loss: 0.6478\nEpoch 21/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 0.8598 - val_loss: 0.6705\nEpoch 22/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 0.8561 - val_loss: 0.6350\nEpoch 23/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 0.8472 - val_loss: 0.6090\nEpoch 24/24\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0065 - val_accuracy: 0.8510 - val_loss: 0.6650\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8682 - loss: 0.5453\nTest Accuracy: 0.8510100841522217\n\u001b[1m174/174\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0022\nTrain Accuracy: 0.9992784857749939\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\nPrecision: 0.756544502617801\nRecall: 0.6689814814814815\nF1-Score: 0.7100737100737101\nAccuracy: 0.851010101010101\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}